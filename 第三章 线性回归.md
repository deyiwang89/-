# 第三章 线性回归
**章节脉络**:![章节脉络](https://uploader.shimo.im/f/wTjIC0R6SqweVxYQ.jpg!thumbnail)

## 3.1基本形式
- 
```math
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
\tag{3.1}
```

## 3.2 线性回归
- 目的是学到`$f(\boldsymbol{x}_{i})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b$`, 使得`$f\left(x_{i}\right) \simeq y_{i}$`
- 对`$\omega, b$`均方最小化,有:
```math
\begin{aligned}\left(w^{*}, b^{*}\right) &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \\ &=\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2} \end{aligned}
\tag{3.4}
```
- 只用最小二乘法进行求解有closed-form解:
```math
w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}
\tag{3.7}
```
```math
b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)
\tag{3.8}
```
- **多元线性回归**
  - `$f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b$`使得`$f\left(x_{i}\right) \simeq y_{i}$`
  - (3.10)求法:令`$E_{\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$`, 展开得:`$E_{\hat{w}}=\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}$`
  - 对`$\hat{\boldsymbol{w}}$`求导有:`$\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=\frac{\partial \boldsymbol{y}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}$`
  - 由向量的求导公式可得：`${\frac{\partial E_{\hat{w}}}{\partial \hat{\boldsymbol{w}}}=0-\mathbf{X}^{T} \boldsymbol{y}-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{\boldsymbol{w}}} $`
  - 既有:`$\frac{\partial E_{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})$`
  - 当`$\mathbf{X}^{T}\mathbf{X}$`正定时, 可求得解:`$\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$`
  - 最终学得线性回归模型`$f\left(\hat{x}_{i}\right)=\hat{x}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$`
## 3.3 对数线性回归
```math
\ln y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
```
- 对数几率回归
  - 二分类任务使用对数几率函数`$y=\frac{1}{1+e^{-z}}$`, 其中`$z=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+\boldsymbol{b}$` 
  - 则有`$\ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$`
  - 将上式中`$y$`视为后验概率可重写为`$\ln \frac{p(y=1 | x)}{p(y=0 | x)}=w^{T} x+b$`
    - 有`$p(y=1 | x)=\frac{e^{w^{\mathrm{T}} x+b}}{1+e^{w^{\mathrm{T}} x+b}}$`
    - `$p(y=0 | \boldsymbol{x})=\frac{1}{1+e^{w^{\mathrm{T}} \boldsymbol{x}+6}}$`
  - 使用maximum likelihood method 估计`$\omega$`和`$b$`有:
    - `$\ell(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} | \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)$`
    - 可得`$l(\beta)=\sum_{i=1}^{m}\left(y_{i} \ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \beta\right)\right)+\left(1-y_{i}\right) \ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \beta\right)\right)\right)$`
    - 有:
```math
\begin{aligned} \frac{\partial l(\beta)}{\partial \beta} &=-\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(y_{i}-\hat{y}_{i}\right) \\ &=\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(\hat{y}_{i}-y_{i}\right) \\ &=\boldsymbol{X}^{T}(\hat{\boldsymbol{y}}-\boldsymbol{y}) \\ &=\boldsymbol{X}^{T}\left(p_{1}(\boldsymbol{X} ; \beta)-\boldsymbol{y}\right) \end{aligned}
\tag{3.30}
```
## 3.4 现行判别分析LDA
- 主要思想: 将样例投影到一条直线上进行聚类, 降维算法.
- 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即`$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}$`尽可能小
- 而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即`$\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}$`尽可能大
- 综合考量, 目标为最大化下式:
```math
\begin{aligned} J &=\frac{\left\|\boldsymbol{w}^{T} \mu_{0}-\boldsymbol{w}^{T} \mu_{1}\right\|_{2}^{2}}{\boldsymbol{w}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \boldsymbol{w}} \\ &=\frac{\left\|\left(\boldsymbol{w}^{T} \mu_{0}-\boldsymbol{w}^{T} \mu_{1}\right)^{T}\right\|_{2}^{2}}{\boldsymbol{w}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \boldsymbol{w}} \\ &=\frac{\left\|\left(\mu_{0}-\mu_{1}\right)^{T} \boldsymbol{w}\right\|_{2}^{2}}{\boldsymbol{w}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \boldsymbol{w}} \\ &=\frac{\boldsymbol{w}^{T}\left(\mu_{0}-\mu_{1}\right)^{T} \boldsymbol{w} ]^{T}\left(\mu_{0}-\mu_{1}\right)^{T} \boldsymbol{w}}{\boldsymbol{w}^{T}\left(\Sigma_{0}+\Sigma_{1}\right) \boldsymbol{w}} \end{aligned}
\tag{3.32}
```
- 定义 类内散度矩阵
  - `$\begin{aligned} \mathbf{S}_{w} &=\mathbf{\Sigma}_{0}+\mathbf{\Sigma}_{1}=\sum_{\boldsymbol{x} \in X_{0}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{0}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{0}\right)^{\mathrm{T}}+\sum_{\boldsymbol{x} \in X_{1}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \end{aligned}$` 
- 定义类间散度矩阵
  - `$\mathbf{S}_{b}=\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}}$`
- 可将(3.32)改写成:
  - `$J=\frac{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}}$`既是LDA的最大化目标.
  - 令`$\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{w} \boldsymbol{w}=1$`有`$\min _{\boldsymbol{w}}-\boldsymbol{w}^{\mathrm{T}} \mathbf{S}_{b} \boldsymbol{w}$`
  - 使用拉格朗日乘子法:
```math
\begin{aligned} l(\boldsymbol{w}) &=-\boldsymbol{w}^{T} \boldsymbol{S}_{b} \boldsymbol{w}+\lambda\left(\boldsymbol{w}^{T} \boldsymbol{S}_{w} \boldsymbol{w}-1\right) \\ \frac{\partial l(\boldsymbol{w})}{\partial \boldsymbol{w}} &=-\frac{\partial\left(\boldsymbol{w}^{T} \boldsymbol{S}_{b} \boldsymbol{w}\right)}{\partial \boldsymbol{w}}+\lambda \frac{\left(\boldsymbol{w}^{T} \boldsymbol{S}_{w} \boldsymbol{w}-1\right)}{\partial \boldsymbol{w}} \\ &=-\left(\boldsymbol{S}_{b}+\boldsymbol{S}_{b}^{T}\right) \boldsymbol{w}+\lambda\left(\boldsymbol{S}_{w}+\boldsymbol{S}_{w}^{T}\right) \boldsymbol{w} \end{aligned}

又\boldsymbol{S}_{b}=\boldsymbol{S}_{b}^{T}, \boldsymbol{S}_{w}=\boldsymbol{S}_{w}^{T}则:

\frac{\partial l(\boldsymbol{w})}{\partial \boldsymbol{w}}=-2 \boldsymbol{S}_{b} \boldsymbol{w}+2 \lambda \boldsymbol{S}_{w} \boldsymbol{w}
\tag{3.37}
```
- 从贝叶斯角度解释(略)
## 3.5 多分类学习
- 来源[CSDN 作者: cjf1699  ](https://blog.csdn.net/cjf1699/article/details/81814325)
- 很多二分类模型可以直接推广到多分类的情况，但是更多情形下，我们是利用二分类器解决多分类问题，这一过程涉及到对多分类任务进行拆分，以及对多分类器进行集成。本节主要介绍了拆分策略：OvO、OvR、MvM.
  - OvO，就是“一对一”。模型在训练时挑选一个类别作为正类，一个类别作为负类，共N个类别时，需要训练N(N-1)/2个分类器。测试阶段，将测试用例喂给每个分类器，在预测结果中选择出现频次最多的作为最终的结果，相当于“投票法”，谁的票数多谁就当选。这种方法的优点是训练时不必用到所有的输入样例，而只需两个类别的样例即可，这在训练集十分庞大时有一定优势。但是它需要训练N²量级个分类器，这导致存储和测试时间的开销比较大。
  - OvR，就是“一对其余”。模型在训练时挑选一个类别作为正类，其余类别全部作为负类，共N个类别时，需要训练N个分类器。测试阶段，将测试用例喂给每个分类器，这时输出情况有两种：（1）只有唯一一个分类器输出为正类，那么此类就是最终的预测结果；（2）有多个分类器输出为正类，则考虑预测置信度，选择预测置信度大的分类作为最终输出。这种方法由于每次要用到全部训练数据，因此训练时间开销较大。
  - MvM，就是“多对多”。模型在训练时挑选若干个类别作为正类，若干个类别作为负类。一种常用的MvM技术叫做ECOC（Error Correcting Output Code）纠错输出码。具体做法是，（编码过程）将N个类别划分M次，在训练集上训练出来M个分类器。也就是说这M次过程之后，每个类别都有一个长度为M的编码，要么0（被划为负类），要么1（被划为正类）；（解码过程）将测试样例分别喂给这M个分类器，得到M个预测标记，组成长度为M的编码，依次计算此编码与每个类别编码的距离，距离最短的作为其类别。之所以叫做“纠错输出码”，是因为这种方法具有一定的容错性能。即使在解码过程中某个分类器出现了错误，依然能产生正确的分类结果。

## 3.6 类别不平衡问题

这一点记得在Ng的视频中没有重点讲解。所谓类别不平衡问题是指训练集内正类数目和负类数目相差悬殊，如正例有998个，而负例只有2个。这样一来一个将所有输入全部预测为正例的无脑分类器也会达到很高的分类准确率，但是却毫无意义。

针对这一问题，有3种解决办法。以下假设正例个数远远大于负例个数。

1. 欠采样。随机抛弃一些正例，使二者数目相当。

2. 过采样。增加一些负例，使二者数目相当。如通过对训练集中的负例进行插值法以产生额外的负例。

3. 阈值转移法。以对数几率回归为例，y/1-y可以看作一种预测几率，即正例可能性和负例可能性的比值。当此几率大于1时（y>0.5）就预测为正例，反之为负例。现在将m+/m-定义为观测几率，m+为训练集中正例数目，m-为负例数目。当训练集是真实样本的无偏估计时（很遗憾，这种情况并不容易达到），观测几率就代表了真实几率。

从而当y/1-y大于m+/m-时，我们再让分类判定为正例，这就是类别不平衡问题的一个基本策略：再缩放。

## Else
关于要求内"Ridge回归、Lasso回归、ElasticNet回归
       Lasso的求解方法(最小角、坐标轴)"等内容,将会在下周进行补充
       
## 习题
转载自 [CSDN 作者:  四去六进一](https://blog.csdn.net/icefire_tyh/article/details/52064910)

- 3.1 两个实例相减可以消除偏置项
- 3.2 还没搞懂, 会尽快证明
- 3.3
- 3.4
- 3.5
- 3.6
- 3.7
- 3.8*
- 3.9
- 3.10*