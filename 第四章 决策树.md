# 第四章 决策树
![](https://uploader.shimo.im/f/gN57ExgV90IGwAK0.png!thumbnail)
## 4.1 基本流程
| 决策树基本算法 |
| ------ |
|输入: `$训练集 D = \left\{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),...,(x_{m},y_{m})\right\}; \\ 属性集 A=\left\{a_{1}, a_{2}, \dots, a_{d}\right\}$`  |
|**过程: 函数TreeGenerate (D,A)**|
|1: 生成结点node;|
|2: **if** *D* 中样本全属于同一类别 *C* **then** |
|3:    将node标记为*C*类叶结点;**return**|
|4: **end if**|
|5: **if** `$A=\varnothing$` **OR** *D* 中样本在 *A* 上取值相同 **then** |
|6: 将node标记为叶结点,其类别标记为 *D* 中样本数量最多的类; **return**    |
|7: **end if**|
|**8: 从 *A* 中选择最优划分属性`$a_*$`**|
|9: **for** `$a_*$`的每个值`$a_{*}^{v}$`**do**|
|10: 为node生成一个分支; 令`$D_v$`表示D中在`$a_*$`上取值为`$a_{*}^{v}$`的样本子集;|
|11: **if** `$D_v$`为空 **then**|
|12: 将分支结点标记为叶结点,其类别标记为D中样本最多的类; **return**|
|13: **else**|
|14: 以TreeGenerate(`$D_v$`, A \ {`$a_*$`})为分支结点|
|15: **end if**|
|16: **end for**|
|**输出:** 以node为根结点的一棵决策树|
- 决策树的生成是一个递归过程, 在决策树基本算法中,有三种情形会导致递归返回:
  - 1. 当前结点包含的样本全属于同一类别,无需划分;
  - 2. 当前属性集为空, 或者所有样本在所有属性上取值相同,无法划分;
  - 3. 当前结点包括的样本集为空, 不能划分.
## 4.2 划分选择
### 4.2.1 信息增益
"信息熵"是度量样本几何纯度最常用的一种指标.熵是度量样本集合纯度最常用的一种指标,代表一个系统中蕴含多少信息量,信息量越大表明一个系统不确定性就越大,就存在越多的可能性. 界定当前样本几何*D*中第*k*类样本所占的比例为`$p_k(k=1,2....,|y|)$`, 则*D*的信息熵定义为:
```math
\operatorname{Ent}(D)=-\sum_{k=1}^{|y|} p_{k} \log _{2} p_{k}
\tag{4.1}
```
`$Ent(D)$`的值越小, 则 *D* 的纯度越高.
- Ent的两种极端情况:
  - 当`$D$`只有一类样本时, Min为0: `$\operatorname{Ent}(D)=-\sum_{k=1}^{|y|} \frac{1}{|y|} \log _{2} \frac{1}{|y|}=-1 \log _{2} 1-0 \log _{2} 0-\ldots-0 \log _{2} 0=0$`
  - 当`$D$`中样本处于均匀分布时,Max为`$\log_{2}{|y|}$`:`$\operatorname{Ent}(D)=-\sum_{k=1}^{|y|} \frac{1}{|y|} \log _{2} \frac{1}{|y|}=\sum_{k=1}^{|y|} \frac{1}{|y|} \log _{2}|y|=\log _{2}|y|$`
 
**信息增益**: 假定在样本`$D$`中有某个**离散特征**`$a$`有`$V$`个可能的取值 `$(a^1,a^2,...,a^V)$`, 若使用特征`$a$`来对样本集`$D$`进行划分, 则会产生`$V$`个分支结点，其中第`$v$`个分支结点包含了`$D$`中所有在特征`$a$`上取值为`$a^v$`的样本, 样本记为`$D^v$`, 由于根据离散特征`$a$`的每个值划分的`$V$`个分支结点下的样本数量不一致, 对于这`$V$`个分支结点赋予权重`$\frac{|D^v|}{|D|}$`, 即样本数越多的分支结点的影响越大, 特征`$a$`对样本集`$D$`进行划分所获得的“信息增益”为:
```math
\operatorname{Gain}(D, a)=E n t(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
```
- **缺点**：由于在计算信息增益中倾向于特征值越多的特征进行优先划分,这样假设某个特征值的离散值个数与样本集`$D$`个数相同(假设为样本编号),虽然用样本编号对样本进行划分, 样本纯度提升最高, 但是并不具有**泛化能力**. 
- 书中例子很好, 值得认真研读

### 4.2.2 增益率
为了提高算法的泛华能力, 使用增益率(gain ratio)来选择最优划分属性.
- 定义
```math
-\text {ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{I V(a)}
```
其中, 
```math
I V(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
```
`$IV(a)$` 是特征 `$a$` 的熵.

增益率对特征值较少的特征有一定偏好, 因此`$C4.5$` 算法选择特征的方法是先从候选特征中选出信息增益高于平均水平的特征, 再从这些特征中选择增益率最高的. 

### 4.2.3 基尼指数
[CART决策树](https://baike.sogou.com/v10880029.htm?fromTitle=cart%EF%BC%88%E7%AE%97%E6%B3%95%E5%90%8D%E7%A7%B0%EF%BC%89)使用基尼指数来选择划分属性. 数据D的纯度可以用基尼值来度量:
```math
\begin{aligned} \operatorname{Gini}(p) &=\sum_{k=1}^{|y|} \sum_{k \neq k^{\prime}} p_{k} p_{k^{\prime}} \\ &=\sum_{k=1}^{|y|} p_{k}\left(1-p_{k}\right) \\ &=1-\sum_{k=1}^{|y|} p_{k}^{2} \end{aligned}
```

直观来说, 基尼指数反应了冲数据集D中随机抽取两个样本, 其类别标记不一致的概率. 因此, Gini(D) 越小, 则数据集D的纯度越高.
- 定义属性a的基尼指数为:
```math
(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
```
于是，我们在候选属性集合A 中，选择那个使得划分后基尼指数最小的属
性作为最优划分属性，即`$a_{*}=\underset{a \in A}{\arg \min } \text { Gini index }(D, a)$`

## 4.3 剪枝处理
决策树剪枝的基本策略有"预剪枝" (prepruning)和"后剪枝"(post"
pruning) [Quinlan, 1993]. 预剪枝是指在决策树生成过程中，对每个结点在划
分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划
分并将当前结点标记为叶结点;后剪枝则是先从训练集生成一棵完整的决策树，
然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能
带来决策树泛化性能提升，则将该子树替换为叶结点.
### 4.3.1 预剪枝
在决策树生成过程中，对每个结点在划分前后进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。
### 4.3.2 后剪枝
在训练出一颗完整的决策树之后，自底向上的对非叶结点进行考察，若将该结点对应的子树替换成叶结点能提升泛化性能，则将该子树替换为叶结点。
### 4.3.3 剪枝过程
使用第二章中提到的性能评估方法来比较泛化性能的改变；
从样本预留出一部分数据用作“验证集”以进行性能评估。
## 4.4 连续与缺失值
### 4.4.1 连续值处理
样本集`$D$`中的连续特征`$a$`, 假设特征`$a$`有`$n$`个不同的取值, 对其进行大小排序, 记为`$\lbrace{a^1,a^2,...,a^n}\rbrace{a^1, a^2,...,a^n}$`, 根据特征`$a$`可得到`$n−1$`个划分点`$t$`, 划分点`$t$`的集合为:
```math
T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leq i \leq n-1\right\}
\tag{4.7}
```
对于取值集合`$T_aT$`中的每个`$t$`值计算将特征`$a$`离散为一个特征值只有两个值, 分别是`$\lbrace{a}>t\rbrace{a>t}$`和`$\lbrace{a}\leq{t}\rbrace{a≤t}$`的特征, 计算新特征的信息增益, 找到信息增益最大的`$t$`值即为该特征的最优划分点:
```math
\begin{aligned} \operatorname{Gain}(D, a) &=\max _{t \in T_{a}} \operatorname{Gain}(D, a) \\ &=\max _{t \in T_{a}} \operatorname{Ent}(D)-\sum_{\lambda \in\{-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} \operatorname{Ent}\left(D_{t}^{\lambda}\right) \end{aligned}
\tag{4.8}
```
遇到连续值属性的一个合适的处理逻辑是：将连续值转换为离散值不就得了。我们需要做的是将训练样本中该属性的所有取值进行排序，并对这排好序的取值队列进行分区划分，每一个分区即为该属性的一个离散点取值。

不失一般性，我们就取二分法（即分为两个分区，可以根据需要分为N个）来进行描述。以身高划分为例，训练集中身高数据取值（cm）排序如下：

｛160，163，168，170，171，174，177，179，180，183｝

因为是二分法，我们只需要找到一个划分点即可。每个划分点可放在两个取值之间，也可放在一个取值之上，这里我们取两个取值之间的中位数。那么上边例子中可选的划分点为：

｛161.5，165.5，169，170.5，172.5，175.5，178，179.5，181.5｝

根据不同的划分，我们可以分别计算一下信息增益的大小，然后选出一个最好的划分来。

需注意的是，和离散值不同，连续值的划分是可以在子结点上继续划分的。即你将身高划分为“小于175”和“大于等于175”两部分，对于“小于175”的子结点，你仍然可以继续划分为“小于160”和“大于160”两部分。
### 4.4.2 缺失值处理

属性缺失时，我们需要处理两个问题：

- **1. 如何在属性值缺失的情况下进行属性划分选择**

不将缺失值的样本代入选择判断的公式计算（信息增益、增益率、基尼指数）之中，只在计算完后乘以一个有值的样本比例即可。

比如训练集有10个样本，在属性 a 上，有两个样本缺失值，那么计算该属性划分的信息增益时，我们可以忽略这两个缺失值的样本来计算信息增益，然后在计算结果上乘以8/10即可。

- **2. 若一个样本在划分属性上的值为空，它应该被分在哪个子结点中**

若样本 x 在划分属性 a 上取值未知，则将 x 划入所有子结点，但是对划入不同子结点中的 x 赋予不同的权值（不同子结点上的不同权值一般体现为该子结点所包含的数据占父结点数据集合的比例）。
## 4.5 多变量决策树
在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，此时的决策树会相当复杂，由于要进行大量的属性测试，预测时间开销会很大。

若能使用斜的划分边界，则决策树模型将大为简化，这就是多变量决策树（multivariate decision tree）。在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试；换言之，每个非叶结点时一个形如线性回归的线性分类器了。
## 重点知识
  1. 信息熵
  2. 信息增益
  3. 增益率
  4. 基尼指数

## 总结

1. 决策树采用的是一种简单且直观的“分而治之”（divide-and-conquer）策略
2. 决策树根据数据集的总体纯度来衡量不同属性划分的优劣
3. 当属性选择过于频繁会导致模型过拟合，这时候就需要使用剪枝算法来进行处理
4. 剪枝算法的判断标准是剪枝前后模型的性能指标是否提高
5. 当遇到连续值的属性时，可以通过将其拆解成两块的方式将数据转换为离散值
6. 当遇到缺失值属性时，可以通过在属性选择算法中加入缺失值数据的影响来适应
7. 在处理真实分类边界比较复杂的学习任务时，我们可以使用属性的线性组合来对数据进行划分
 

## 习题
