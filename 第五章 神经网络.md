## 5.1 神经元模型
![](https://upload-images.jianshu.io/upload_images/11455432-a90edae90283dc50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/393/format/webp)

输出:`$y=f\left(\sum_{i=1}^{n} w_{i} x_{i}-\theta\right)$`; 

在我的理解中, **激活函数**使得神经元与线性模型产生区别.神经元就像是砖瓦, 而神经网络更像是建筑, 神经元的堆叠和排列决定了建筑的功效
## 5.2 感知机与多层网络
- 感知机能够实现 与 或 非 三种逻辑运算

- `$\Delta w_i = \eta(y-\hat{y})x_i$`

[推导]：此处感知机的模型为： `$y=f(\sum_{i} w_i x_i - \theta)$` 

将`$\theta$`看成哑结点后，模型可化简为： 
```math
y=f(\sum_{i} w_i x_i)=f(\boldsymbol w^T \boldsymbol x)
```
其中`$f$`为阶跃函数。

根据《统计学习方法》§2可知，假设误分类点集合为`$M$`，`$\boldsymbol x_i \in M$`为误分类点，`$\boldsymbol x_i$`的真实标签为`$y_i$`, 模型的预测值为`$\hat{y_i}$`, 对于误分类点`$\boldsymbol x_i$`来说，此时`$\boldsymbol w^T \boldsymbol x_i \gt 0,\hat{y_i}=1,y_i=0$`或`$\boldsymbol w^T \boldsymbol x_i \lt 0,\hat{y_i}=0,y_i=1$`, 综合考虑两种情形可得：  `$(\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i>0$` 

所以可以推得损失函数为： `$L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol w \boldsymbol x_i$` 

损失函数的梯度为： `$\nabla_w L(\boldsymbol w)=\sum_{\boldsymbol x_i \in M} (\hat{y_i}-y_i)\boldsymbol x_i$` 

随机选取一个误分类点`$(\boldsymbol x_i,y_i)$`，对`$\boldsymbol w$`进行更新： `$\boldsymbol w \leftarrow \boldsymbol w-\eta(\hat{y_i}-y_i)\boldsymbol x_i=\boldsymbol w+\eta(y_i-\hat{y_i})\boldsymbol x_i$` 

显然式5.2为`$\boldsymbol w$`的第`$i$`个分量`$w_i$`的变化情况

`$\eta$`为学习率

- 为了解决更复杂的问题, 需要构建多层结构, 最初期的神经网络就是**多层前馈神经网络**
## 5.3 误差逆传播算法
- BP网络

假定神经网络的输出为`$\hat{\boldsymbol{y}}_{k}=\left(\hat{y}_{1}^{k}, \hat{y}_{2}^{k}, \ldots, \hat{y}_{l}^{k}\right)$`, 即
```math
\hat{y}_{j}^{k}=f\left(\beta_{j}-\theta_{j}\right)
```
则网络在`$\left(x_{k}, y_{k}\right)$`上的RMSE为:
```math
E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}^{k}\right)^{2}
\tag{5.4}
```
![](https://img-blog.csdn.net/20181001153452332?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NuYWlsYnVzdGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图中有`$p = (d+l)q+q+l$`个参数需要确定。

BP算法基于梯度下降策略， 对于式(5.4), 有:
```math
\Delta w_{h j}=-\eta \frac{\partial E_{k}}{\partial w_{h j}}
\tag{5.6}
```
根据链式法则有:
```math
\frac{\partial E_{k}}{\partial w_{h j}}=\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial w_{h j}}
\tag{5.7}
```
根据`$ \beta_{j}$`的定义, 有
```math
\frac{\partial \beta_{j}}{\partial w_{h j}}=b_{h}
\tag{5.8}
```
根据sigmoid函数的性质:`$f^{\prime}(x)=f(x)(1-f(x))$`
于是可有:
```math
\begin{aligned} g_{j} &=-\frac{\partial E_{k}}{\partial \hat{y}_{j}^{k}} \cdot \frac{\partial \hat{y}_{j}^{k}}{\partial \beta_{j}} \\ &=-\left(\hat{y}_{j}^{k}-y_{j}^{k}\right) f^{\prime}\left(\beta_{j}-\theta_{j}\right) \\ &=\hat{y}_{j}^{k}\left(1-\hat{y}_{j}^{k}\right)\left(y_{j}^{k}-\hat{y}_{j}^{k}\right) \end{aligned}
\tag{5.10}
```
式(5.10)+(5.8)带入(5.7)再带入(5.6), 可以得到BP算法中`$ w_{h j}$`的更新公式:
```math
\Delta w_{h j}=\eta g_{j} b_{h}
\tag{5.11}
```
[推导]：因为 
```math
\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}
```
又 
```math
\begin{aligned} \cfrac{\partial E_k}{\partial \theta_j} &= \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot\cfrac{\partial \hat{y}_j^k}{\partial \theta_j} \ \\&= (\hat{y}_j^k-y_j^k) \cdot f’(\beta_j-\theta_j) \cdot (-1) \\ &= -(\hat{y}_j^k-y_j^k)f’(\beta_j-\theta_j) \\ &= g_j \end{aligned}
```
所以 
```math
\Delta \theta_j = -\eta \cfrac{\partial E_k}{\partial \theta_j}=-\eta g_j
\tag{5.12}
```



[推导]：因为 `$\Delta v_{ih} = -\eta \cfrac{\partial E_k}{\partial v_{ih}}$`

又
```math
\begin{aligned} \cfrac{\partial E_k}{\partial v_{ih}} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot \cfrac{\partial \alpha_h}{\partial v{ih}} \\ &= \sum{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \alpha_h} \cdot x_i \\ &= \sum{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\ &= \sum{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot w{hj} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\ &= \sum{j=1}^{l} (-g_j) \cdot w{hj} \cdot f’(\alpha_h-\gamma_h) \cdot x_i \\ &= -f’(\alpha_h-\gamma_h) \cdot \sum{j=1}^{l} g_j \cdot w_{hj} \cdot x_i\ \\&= -b_h(1-b_h) \cdot \sum_{j=1}^{l} g_j \cdot w_{hj} \cdot x_i \\ &= -e_h \cdot x_i \end{aligned}
```
所以 
```math
\Delta v_{ih} = -\eta \cdot -e_h \cdot x_i=\eta e_h x_i
\tag{5.13}
```



[推导]：因为 `$\Delta \gamma_h = -\eta \cfrac{\partial E_k}{\partial \gamma_h}$` 

又
```math
\begin{aligned} \cfrac{\partial E_k}{\partial \gamma_h} &= \sum_{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot \cfrac{\partial b_h}{\partial \gamma_h} \\ &= \sum{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot \cfrac{\partial \beta_j}{\partial b_h} \cdot f’(\alpha_h-\gamma_h) \cdot (-1) \\ &= -\sum{j=1}^{l} \cfrac{\partial E_k}{\partial \hat{y}_j^k} \cdot \cfrac{\partial \hat{y}j^k}{\partial \beta_j} \cdot w{hj} \cdot f’(\alpha_h-\gamma_h)\\ &=e_h \end{aligned} 
```

所以 
```math
\Delta \gamma_h= -\eta e_h
\tag{5.14}
```
在(5.13), (5.14)中有:
```math
\begin{aligned} e_{h} &=-\frac{\partial E_{k}}{\partial b_{h}} \cdot \frac{\partial b_{h}}{\partial \alpha_{h}} \\ &=-\sum_{j=1}^{l} \frac{\partial E_{k}}{\partial \beta_{j}} \cdot \frac{\partial \beta_{j}}{\partial b_{h}} f^{\prime}\left(\alpha_{h}-\gamma_{h}\right)\\ &=\sum_{j=1}^{l} w_{h j} g_{j} f^{\prime}\left(\alpha_{h}-\gamma_{h}\right) \\ &=b_{h}\left(1-b_{h}\right) \sum_{j=1}^{l} w_{h j} g_{j} \end{aligned}
\tag{5.15}
```

误差逆传播算法:
```
输入: 训练集D = {(x_k,y_k)};
      学习率 = \eta.
过程:
在(0,1)范围内随机初始化网络中所有连接权和阈值
repeat
    for all (x,y) belong to D do
      based on parameter and (5.3) calculate the output y^k;
      calculate output gratitude g_j through (5.10)
      calculate the gratitude of hidden layer e_h;
      renew the weight and threshold
    end for
until satisfies target
output: network with fixed parameters
```

## 5.4 全局最小与局部最小
- adagrad
- momentum
- Adam
## 5.5 其他神经网络
- RBF
- ART
- SOM
- CASCADE-CORRELATION
- ELMAN
- BOLTZMANN
## DEEP LEARNING