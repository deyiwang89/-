### 要求:
-  过拟合与欠拟合及其解决方法
-  评估学习器的方法
-  如何度量模型性能
-  方差——偏差分解
-  比较检验
-  
部分内容来源于 [Element简的笔记](https://blog.csdn.net/yanyiting666/article/details/93381283)
### 名词解释:
- 错误率（error rate）：分类错误的样本数占样本总数的比例
- 精度（accuracy）：精度 = 1 - 错误率
- 误差（error）：学习器的实际预测输出与样本的真实输出之间的差异
- 训练误差（training error）/经验误差（empirical error）：学习器在训练集上的误差
- 泛化误差（generalization error）：学习器在新样本上的误差
- 过拟合（overfitting）：学习器把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降
- 欠拟合（underfitting）：学习器对训练样本的一般性质尚未学好

### 评估方法:

如果我们还有一个包含m个样例的数据集D={(x1,y1),(x2,y2),...,(xm,ym)}，既要训练，又要测试，就需要通过对D进行适当的处理，从中产生出训练集S和测试集T。

1. 留出法 （hold-out）

2. 交叉验证法（cross validation）

3. 自助法 （bootstrapping）



### 性能度量（performance measure）:衡量模型泛化能力的评价标准。


- 错误率与精度

分类任务中最常用的两种性能度量，其中错误率是分类错误的样本数占样本总数的比例。精度是分类正确的样本数占样本总数的比例。

- 查准率、查全率与F1

查准率（precision）：被模型预测为正的样本中，真正为正的样本比例（被预测为好瓜的西瓜中，真的是好瓜的西瓜比例）

查全率（recall）：在所有正的样本上，被学习器预测为正的样本比例（在所有好瓜中，被学习器正确识别的好瓜比例）

F1：（2x查准率x查全率）/（查准率＋查全率）。 F1是基于查准率和查全率的调和平均。

（查准率和查全率的概念不难理解，但是意思比较相近，看过一段时间之后老是会弄混。在读论文时，很多时候是用precision和recall来进行性能度量，所以将熟记这两个指标还是十分必要的）

平衡点（Break-Even Point，简称BEP）：为了在PR图中识别学习器的性能谁更优异，人们设计了一些综合考虑查准率、查全率的性能度量。平衡点就是其中之一，它是查准率=查全率时的取值。平衡点的取值越大，学习器越优。

- ROC与AUC

ROC：“受试者工作特征”（receiver operating characteristic）曲线，与P-R 曲线类似，只不过取用“真正利率”（TPR ）作为纵轴，“假正例率”（FPR）作为横轴

- 代价敏感错误率与代价曲线

代价敏感错误率代表数据的平均总体代价；代价曲线详情见书本

### 假设检验

基于假设检验结果我们可推断出，若在测试集上观察到学习器A比B好，则A的泛化性能是否在统计意义上优于B，以及这个结论的把握有多大。

### 偏差与方差

偏差－方差分解是对学习算法的期望泛化错误率进行的分解。泛化误差可分解为偏差、方差与噪声之和。

其中，偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所带来的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。
