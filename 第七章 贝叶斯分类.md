# Task 5--第七章 贝叶斯分类
- 重点掌握：(标注`$^*$`说明还没看懂)
  - 极大似然估计与朴素贝叶斯
  - 半朴素贝叶斯
  - 叶斯网`$^*$`
  - EM算法`$^*$`
## 7.1 贝叶斯决策论
假设有N种可能的类别标记, 即`$\mathcal{Y}=\left\{c_{1}, c_{2}, \ldots, c_{N}\right\}$`, `$\lambda_{ij}$`是将一个真实标记为`$c_{j}$`的样本误分类为`$c_{i}$`所产生的期望损失, 及在样本x上的"条件风险":
```math
R\left(c_{i} | \boldsymbol{x}\right)=\sum_{j=1}^{N} \lambda_{i j} P\left(c_{j} | \boldsymbol{x}\right)
\tag{7.1}
```
我们的目标是找到一个判定准则`$h : \mathcal{X} \mapsto \mathcal{Y}$`进而最小化总体风险
```math
R(h)=\mathbb{E}_{\boldsymbol{x}}[R(h(\boldsymbol{x}) | \boldsymbol{x})]
\tag{7.2}
```
所以, 目标就是最小化条件风险`$R(h(x) | x)$`. 也因此产生贝叶斯判定准则:**为最小化总体风险, 只需要在每个样本上选择哪个能是条件风险`$R(c|x)$`最小的类别标记即:**
```math
h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \min } R(c | \boldsymbol{x})
\tag{7.3}
```
为了最小化分类错误率, 使用0-1作为误判损失`$\lambda_{ij}$`则条件风险为:
```math
R(c|\boldsymbol x)=1−P(c|\boldsymbol x)
\tag{7.5}
```

[推导]：由式(7.1)和式(7.4)可得： 

```math
R(c_i|\boldsymbol x)=1P(c_1|\boldsymbol x)+1P(c_2|\boldsymbol x)+...+0P(c_i|\boldsymbol x)+...+1P(c_N|\boldsymbol x)
```

又 `$\sum_{j=1}^{N}P(c_j|\boldsymbol x)=1$`，则： `$R(c_i|\boldsymbol x)=1-P(c_i|\boldsymbol x)$` 此即为式(7.5)
于是, 最小化分类错误率的贝叶斯最优分类器为:
```math
h^{*}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c | \boldsymbol{x})
\tag{7.6}
```
即对每个样本x,选择能使后欧燕概率`$P(c|x)$`最大的类别标记.

如果使用贝叶斯判定准则类最小化决策风险, 就需要得到后验概率`$P(c|x)$`显然这是不现实的. 实际上, 机器学习的本质是通过有线的训练样本尽可能的准确估计出后验概率`$P(c|x)$`. 大体来说，主要有两种策略:给定x , 可通过直接建模`$P(c|x)$`来
预测c，这样得到的是"判别式模型" (discriminative models); 也可先对联合概率分布`$P(c,x)$`建模，然后再由此获得`$P(c|x)$`, 这样得到的是"生成式模
型" (generative models) 显然，前面介绍的决策树、BP 神经网络、支持向量
机等，都可归入判别式模型的范畴. 则有:
```math
P(c | \boldsymbol{x})=\frac{P(\boldsymbol{x}, c)}{P(\boldsymbol{x})}=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}
\tag{7.7-7.8}
```
 [解析]：最小化误差，也就是最大化`$P(c|x)$`，但由于`$P(c|x)$`属于后验概率无法直接计算，由贝叶斯公式可计算出: `$P(c|\boldsymbol x)=\cfrac{P(c)P(\boldsymbol x|c)}{P(\boldsymbol x)}$` `$P(\boldsymbol x)$` 
 可以省略，因为我们比较的时候
 `$P(\boldsymbol x)$`一定是相同的，所以我们就是用历史数据计算出`$P(c)$`和`$P(\boldsymbol x|c)$`。

`$P(c)$`根据大数定律，当样本量到了一定程度且服从独立同分布，c的出现的频率就是c的概率。

`$P(\boldsymbol x|c)$`，因为`$\boldsymbol x$`在这里不对单一元素是个矩阵，涉及n个元素，不太好直接统计分类为c时，`$\boldsymbol x$`的概率，所以我们根据假设独立同分布，对每个`$\boldsymbol x$`的每个特征分别求概率 
```math
P(\boldsymbol x|c)=P(x_1|c)*P(x_2|c)*P(x_3|c)...*P(x_n|c)
```
这个式子就可以很方便的通过历史数据去统计了,比如特征n，就是在分类为c时特征n出现的概率，在数据集中应该是用1显示。 但是当某一概率为0时会导致整个式子概率为0，所以采用拉普拉斯修正
当样本属性独依赖时，也就是除了c多加一个依赖条件，式子变成了 `$∏_{i=1}^n P(x_i|c,p_i)$` `$p_i$`是`$x_i$`所依赖的属性

当样本属性相关性未知时,我们采用贝叶斯网的算法，对相关性进行评估，以找出一个最佳的分类模型。

当遇到不完整的训练样本时，可通过使用EM算法对模型参数进行评估来解决。
## 7.2 极大似然估计
参见:[知行流浪的博客](https://blog.csdn.net/zengxiantao1994/article/details/72787849)
## 7.3 朴素贝叶斯分类器
基于属性条件独立假设, 式(7.8)可以重写为:
```math
P(c | \boldsymbol{x})=\frac{P(c) P(\boldsymbol{x} | c)}{P(\boldsymbol{x})}=\frac{P(c)}{P(\boldsymbol{x})} \prod_{i=1}^{d} P\left(x_{i} | c\right)
\tag{7.14}
```
其中d为属性数据, `$x_i$`为x在第i个属性上的取值.
由于对所有类别来说P(x) 相同，因此基于式(7.6) 的贝叶斯判定准则有:
```math
h_{n b}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right)
\tag{7.15}
```
即为朴素贝叶斯分类器的表达式.

```math
P\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|}{\left|D_{c}\right|}
\tag{7.17}
```
```math
p\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)
\tag{7.18}
```
`$P_{(\boldsymbol x_{i}|c)}\in[0,1]$` `$p_{(\boldsymbol x_{i}| c)}$`


[解析]：式(7.17)所得`$P_{(\boldsymbol x_{i}|c)}\in[0,1]$`为条件概率，但式(7.18)所得`$p_{(\boldsymbol x_{i}| c)}$`为条件概率密度而非概率，其值并不在局限于区间[0,1]之内。

为了避免其他属性携带的信息被训练集中未出现的属性值"抹去'气
在估计概率值时通常要进行"平滑" (smoothing) ，常用"拉普拉斯修
正" (Laplacian correctio叫具体来说，令N 表示训练集D 中可能的类别
数， N; 表示第4 个属性可能的取值数，则式(7.16) 和(7.17) 分别修正为:
```math
\hat{P}(c)=\frac{\left|D_{c}\right|+1}{|D|+N}
\tag{7.19}
```

```math
\hat{P}\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|+1}{\left|D_{c}\right|+N_{i}}
\tag{7.20}
```
## 7.4 半朴素贝叶斯分类器
独依赖估计--假设每个属性在类别之外最多仅依赖于其他一个属性:
```math
P(c | \boldsymbol{x}) \propto P(c) \prod_{i=1}^{d} P\left(x_{i} | c, p a_{i}\right)
\tag{7.21}
```
其中`$pa_i$`为属性`$x_i$` 所依赖的属性，称为`$x_i$`的父属性.此时，对每个属性`$x_i$`，若
其父属性`$pa_i$`知，则可采用类似式(7.2)的办法估计概率值`$P(x_i|c,pa_i)$`于是, 问题的关键就转化为如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器. 



7.23

`$P(c|\boldsymbol x)\propto{\sum_{i=1 \atop |D_{x_{i}}|\geq m'}^{d}}P(c,x_{i})\prod_{j=1}^{d}P(x_j|c,x_i)$`

[推导]： 
```math
\begin{aligned} P(c|\boldsymbol x)&=\cfrac{P(\boldsymbol x,c)}{P(\boldsymbol x)}\ &=\cfrac{P\left(x_{1}, x_{2}, \ldots, x_{d}, c\right)}{P(\boldsymbol x)}\ &=\cfrac{P\left(x_{1}, x_{2}, \ldots, x_{d} | c\right) P(c)}{P(\boldsymbol x)} \ &=\cfrac{P\left(x_{1}, \ldots, x_{i-1}, x_{i+1}, \ldots, x_{d} | c, x_{i}\right) P\left(c, x_{i}\right)}{P(\boldsymbol x)} \ \end{aligned}
```
```math
\begin{aligned} P(c|\boldsymbol x)&\propto P(c,x_{i})P(x_{1},…,x_{i-1},x_{i+1},…,x_{d}|c,x_{i}) \ &=P(c,x_{i})\prod {j=1}^{d}P(x_j|c,x_i) \end{aligned}
```
```math
P(c|\boldsymbol x)\propto\sum\limits{i=1 \atop |D_{x_{i}}|\geq m'}^{d}P(c_{i}|\boldsymbol x_{i})\prod_{j=1}^{d}P(c_{i}|\boldsymbol x_{i})
```
此即为式7.23，由于式(7.24)和式(7.25)的使用到了`$|D_{c,x_{i}}|$`与`$|D_{c,x_{i},x_{j}}|$`，若`$|D_{x_{i}}|$`集合中样本数量过少，则`$|D_{c,x_{i}}|$`与`$|D_{c,x_{i},x_{j}}|$`将会更小，因此在式(7.23)中要求`$|D_{x_{i}}|$`集合中样本数量不少于`$m'$`。




sklearn调包
```python
 import numpy as np
 X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
 Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
 clf = GaussianNB()
clf.fit(X, Y)
GaussianNB(priors=None, var_smoothing=1e-09)
print(clf.predict([[-0.8, -1]]))

参数:

priors : array-like, shape (n_classes,) Prior probabilities of the classes. If specified the priors are not adjusted according to the data.

var_smoothing : float, optional (default=1e-9) Portion of the largest variance of all features that is added to variances for calculation stability.
```

## 7.5 贝叶斯网络`$^*$`
参见 [贝叶斯网络（belief network）及相关知识整理](https://blog.csdn.net/Pancheng1/article/details/81001459) 比较详细


## 7.6 EM`$^*$`
参见 [EM算法及算例详解](https://blog.csdn.net/qq_16000815/article/details/80384024)


## 贝叶斯应用

中文分词 分词后，得分的假设是基于两词之间是独立的，后词的出现与前词无关
统计机器翻译 统计机器翻译因为其简单，无需手动添加规则，迅速成为了机器翻译的事实标准。
贝叶斯图像识别 首先是视觉系统提取图形的边角特征，然后使用这些特征自底向上地激活高层的抽象概念，然后使用一个自顶向下的验证来比较到底哪个概念最佳地解释了观察到的图像。