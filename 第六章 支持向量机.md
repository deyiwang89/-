# 支持向量机
- 部分资料参考luckmia的CSDN博客:[SVM支持向量机入门及数学原理](https://blog.csdn.net/qq_35992440/article/details/80987664)
- 间隔最大化支持向量机
![](https://img-blog.csdn.net/20180328155347956?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzIwMTc3MzI3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：

- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机；
- 当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机；
- 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机；

## 6.1 间隔与支持向量
- 超平面:`$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b=0$`
- 样本空间任意一点到超平面的距离推导:

超平面上任意一点`$x^{\prime}$`显然满足:`$ \omega^{\mathrm{T}} \boldsymbol{x^{\prime}}+b=0$`

对于空间上任意一点`$x^{\prime}$`, `$x^{\prime}$`到平面 `$A$`的距离`$H$`, 等于`$x$`到超平面的法向量长度, 也就是 向量`$xx^{\prime}$` 在垂直方向上(即法向量)上的投影. 而计算投影, 将`$xx^{\prime}$`乘以法向量`$ \omega$` 即可. 并且, 我们不光要投影, 还要计算单位, 即使用单位为`$1$`的投影. 也就是在分母除以`$| | \omega| |$`. 所以，距离`$H$`可以表示为:
```math
d=\left|\frac{\omega^{T}}{| | \omega| |}\left(x-x^{\prime}\right)\right|
```
又由:
```math
\omega^{T} x^{\prime}=-b
```
有
```math
d=\frac{\left|w^{T}\left(x-x^{\prime}\right)\right|}{\|w\|}=\frac{w^{T} x+b}{\|w\|}
```
则类别可按照如下规则进行分类:
```math
\left\{\begin{array}{ll}{W^{T} x_{i}+b \geq+1} & {y_{i}=+1} \\ {W^{T} x_{i}+b \leq-1} & {y_{i}=-1}\end{array}\right.
\tag{6.3}
```
- 支持向量
上图中距离超平面最近的几个训练样本使得(6.3)成立即为"支持向量". 两个异类支持向量到超平面的距离之和为`$\gamma=\frac{\left(\vec{x}_{+}-\vec{x}_{-}\right) \vec{W}^{T}}{\|W\|}=\frac{1-b+1+b}{\|W\|}=\frac{2}{\|W\|}$`称之为**间隔**.
- 因此, 目标变成找到最大间隔的超平面:
```math
\begin{array}{l}{\max \frac{2}{\|W\|}} \\ {\text {s.t.y}_{i}\left(W^{T} x_{i}+b\right) \geq+1}\end{array}
\tag{6.5}
```
显然, 为了最大化间隔, 仅需最大化`$\frac{1}{\|W\|}$`,等价于最小化`${\|W\|}^{2}$`于是, 式(6.5)可以改写为:
```math
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}
\tag{6.6}
```
式(6.6)既是支持向量机的基本型.
## 6.2对偶问题
- 对应模型
```math
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+\boldsymbol{b}
\tag{6.7}
```
- 拉格朗日乘子法, [马同学学数学](https://www.zhihu.com/question/38586401/answer/457058079)讲得很好. 可将式(6.6)的对偶问题改写为拉格朗日函数:
```math
L(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})=f(\boldsymbol{x})+\boldsymbol{\lambda} h(\boldsymbol{x})+\boldsymbol{\mu} g(\boldsymbol{x})
```
```math
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right)\right)
\tag{6.8}
```
- 对式(6.8)进行展开:
```math
\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) &=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right) \\ &=\frac{1}{2}\|\boldsymbol{w}\|^{2}+\sum_{i=1}^{m}\left(\alpha_{i}-\alpha_{i} y_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\alpha_{i} y_{i} b\right) \\ &=\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}+\sum_{i=1}^{m} \alpha_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{w}^{T} \boldsymbol{x}_{i}-\sum_{i=1}^{m} \alpha_{i} y_{i} b \end{aligned}
```
分别对`$\omega$`和`$b$`求偏导并置为`$0$`有:
```math
\begin{aligned} \frac{\partial L}{\partial \boldsymbol{w}}=\frac{1}{2} \times 2 \times \boldsymbol{w}+0-\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}-0=0 & \Longrightarrow \boldsymbol{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\ \frac{\partial L}{\partial b}=0+0-0-\sum_{i=1}^{m} \alpha_{i} y_{i}=0 & \Longrightarrow \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \end{aligned}
```
将上式带入(6.8)后, 有:
```math
\begin{aligned} \min _{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \boldsymbol{\alpha}) &=-\frac{1}{2} \boldsymbol{w}^{T} \sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}+\sum_{i=1}^{m} \alpha_{i} \\ &=-\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)^{T}\left(\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}\right)+\sum_{i=1}^{m} \alpha_{i} \\ &=-\frac{1}{2} \sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}^{T} \sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i}+\sum_{i=1}^{m} \alpha_{i} \\ &=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{j} \end{aligned}
\tag{6.11}
```
可得到:
```math
\max _{\boldsymbol{\alpha}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{i} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}
```
```math
\begin{array}{c}{\sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
```
求解出`$\alpha$`后, 最终得到:
```math
f(x)=w^{T} x+b=\sum_{i=1}^{m} \alpha_{i} y_{i} x_{i}^{T} x+b
```
该过程的`$KKT$`过程为:
```math
\left\{\begin{array}{c}{\alpha_{i} \geq 0} \\ {y_{i} f\left(x_{i}\right)-1 \geq 0} \\ {\alpha_{i}\left(y_{i} f\left(x_{i}\right)-1\right)=0}\end{array}\right.
\tag{6.13}
```
对于任意的训练样本`$(x_i,y_i)$`:
- 若`$\alpha_i=0$`，则其不会在公式（6.13）中的求和项中出现，也就是说，它不影响模型的训练；
- 若 `$\alpha_i>0$`，则`$y_if(x_i)−1=0$`，也就是 则`$y_if(x_i)=1$`，即该样本一定在边界上，是一个支持向量。

这里显示出了支持向量机的重要特征：当训练完成后，大部分样本都不需要保留，最终模型只与支持向量有关。

## 6.3 核函数
对于非线性问题, 线性可分支持向量机并不能有效解决, 要使用非线性模型才能很好地分类. 因此, 需要将原始样本`$x$`通过`$\phi(x)$`映射到高维空间中, 则模型可以表示为:
```math
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b
\tag{6.19}
```
同式(6.6)有:
```math
\begin{array}{l}{\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}} \\ {\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \phi\left(\boldsymbol{x}_{i}\right)+\boldsymbol{b}\right) \geqslant 1, \quad i=1,2, \ldots, m}\end{array}
\tag{6.20}
```
其对偶问题:
```math
\max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)
\tag{6.21}
```
```math
\begin{array}{l}{\sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
```
在式(6.21)中如果直接计算`$\phi{x_i}^T\phi{x_i}$`是极其困难的 (因为很可能映射到无穷维), 在此引入核方法:
```math
\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)
\tag{6.22}
```
于是, 有:
```math
\begin{array}{l}{\max _{\alpha} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \kappa\left(x_{i}, x_{j}\right)} \\ {\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}
\tag{6.23}
```
```math
\begin{aligned} f(\boldsymbol{x}) &=\boldsymbol{w}^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi(\boldsymbol{x})+b \\ &=\sum_{i=1}^{m} \alpha_{i} y_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b \end{aligned}
\tag{6.24}
```
- **定理 6.1** 令`$\mathcal{X}$`为输入空间,`$k(.,.)$`是定义在`$\mathcal{X}\times\mathcal{X}$`上的对称函数, 则`$k$`是核函数当且仅当对于任意数据`$D=\left\{\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \ldots, \boldsymbol{x}_{m}\right\}$`, 核矩阵**K**总是半正定的:


```math
\mathbf{K}=\left[ \begin{array}{cccc}{\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{m}\right)} \\ {\vdots} & {\ddots} & {\vdots} & {\ddots} & {\vdots} \\ {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{1}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{j}\right)} & {\cdots} & {\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{m}\right)}\end{array}\right]
```
- 定理6.1 表明，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用.事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射`$\phi$`.换言之，任何一个核函数都隐式地定义了一个称为"再生核希尔伯特空间" (Reproducing Kernel Hilbert Space ，简称RKHS) 的特征空间.
- 通过前面的讨论可知，我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要.需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地走义了这个特征空间.于是，**"核函数选择"成为支持向量机的最大变数**. 若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳.

| 名称 | 表达式 | 参数 |
| ------ | ------ | ------ |
| 线性核 | `$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}$` |  |
| 多项式核|`$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}\right)^{d}$`|`$d \geqslant 1$`为多项式的次数|
| 高斯核|`$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|^{2}}{2 \sigma^{2}}\right)$`|`$\sigma>0$`为高斯核的带宽(width)|
| 拉普拉斯核|`$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|}{\sigma}\right)$`|`$\sigma>0$`|
| sigmoid核|`$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\tanh \left(\beta \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}+\theta\right)$`|`$tanh$`为双曲正切函数, `$\beta>0, \theta<0$`|

除此之外, 还可以通过函数组合得到核函数:
- 线性组合: `$\gamma_{1} \kappa_{1}+\gamma_{2} \kappa_{2}$`
- 直积: `$\kappa_{1} \otimes \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) \kappa_{2}(\boldsymbol{x}, \boldsymbol{z})$`
- 任意核函数`$g(x)$`: `$\kappa(\boldsymbol{x}, \boldsymbol{z})=g(\boldsymbol{x}) \kappa_{1}(\boldsymbol{x}, \boldsymbol{z}) g(\boldsymbol{z})$`
- 
## 6.4 软间隔与正则化
- 软间隔
实际操作中难以确定完美的核函数, 允许某些样本不满足约束, `$y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1$`
当然，在最大化间隔的同时，不满足约束的样本应尽可能少.于是，优化目标可写为:
```math
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right)
\tag{6.29}
```
其中`$C>0$`是一个常数, `$\ell_{0 / 1}$`是"0/1损失函数"
```math
\ell_{0 / 1}(z)=\left\{\begin{array}{ll}{1,} & {\text { if } z<0} \\ {0,} & {\text { otherwise }}\end{array}\right.
\tag{6.30}
```
然而, `$\ell_{0 / 1}$`非凸、非连续，数学性质不太好，使得式(6.29)不易直接求解. 于是，人们通常用其他一些函数来代替`$\ell_{0 / 1}$`，称为"替代损失" (surrogate loss).
- hinge loss: `$\ell_{\text {hinge}}(z)=\max (0,1-z)$`
- 指数loss: `$\ell_{e x p}(z)=\exp (-z)$`
- logistic loss: `$\ell_{\log }(z)=\log (1+\exp (-z))$`

若采用hinge loss, 有:
```math
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \max \left(0,1-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)
\tag{6.34}
```
引入松弛变量后, 可改写为:
```math
\min _{\boldsymbol{w}, b, \xi_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}
\tag{6.35}
```
```math
\begin{array}{c}{\text { s.t. } y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}} \\ {\xi_{i} \geqslant 0, i=1,2, \ldots, m}\end{array}
```
通过拉格朗日乘子法得到拉格朗日函数:
```math
\begin{aligned} L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \boldsymbol{\xi}, \boldsymbol{\mu})=& \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i} \\ &+\sum_{i=1}^{m} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i} \end{aligned}
\tag{6.36}
```
可得:
```math
\begin{aligned} \boldsymbol{w} &=\sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\ 0 &=\sum_{i=1}^{m} \alpha_{i} y_{i} \\ C &=\alpha_{i}+\mu_{i} \end{aligned}
```
得到对偶问题:
```math
\begin{aligned} \max _{\boldsymbol{\alpha}} & \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \\ \text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\ & 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \ldots, m \end{aligned}
```
## 6.5 支持向量回归 SVR
- 添加容忍偏差`$\epsilon$`, 这相当于以f(x)为中心, 构建了一个宽度为`$2\epsilon$`的问隔带，若
训练样本落入此间隔带，则认为是被预测正确的.
于是SVR问题可以形式化为:
```math
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{c}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)
\tag{6.43}
```
引入松弛变量`$\xi_{i}$`, `$\hat{\xi}_{i}$`后(6.43)可以转化为:
```math
\min _{\boldsymbol{w}, b, \xi_{i}, \hat{\xi}_{i}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)
\tag{6.45}
```
```math
\begin{aligned} \text { s.t. } & f\left(\boldsymbol{x}_{i}\right)-y_{i} \leqslant \epsilon+\xi_{i} \\ & y_{i}-f\left(\boldsymbol{x}_{i}\right) \leqslant \epsilon+\hat{\xi}_{i} \\ & \xi_{i} \geqslant 0, \hat{\xi}_{i} \geqslant 0, i=1,2, \ldots, m \end{aligned}
```
由拉格朗日乘子法得到(6.45)的拉式函数:
```math
\begin{array}{l}{L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}}, \boldsymbol{\mu}, \hat{\boldsymbol{\mu}})} \\ {=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\hat{\xi}_{i}\right)-\sum_{i=1}^{m} \mu_{i} \xi_{i}-\sum_{i=1}^{m} \hat{\mu}_{i} \hat{\xi}_{i}} \\ {+\sum_{i=1}^{m} \alpha_{i}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)+\sum_{i=1}^{m} \hat{\alpha}_{i}\left(y_{i}-f\left(\boldsymbol{x}_{i}\right)-\epsilon-\hat{\xi}_{i}\right)}\end{array}
\tag{6.46}
```
求偏导置零后可得到对偶问题:
```math
\begin{aligned} \max _{\boldsymbol{\alpha}, \boldsymbol{\alpha}} & \sum_{i=1}^{m} y_{i}\left(\hat{\alpha}_{i}-\alpha_{i}\right)-\epsilon\left(\hat{\alpha}_{i}+\alpha_{i}\right) \\ &-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right)\left(\hat{\alpha}_{j}-\alpha_{j}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \\ \text { s.t. } & \sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right)=0 \\ & 0 \leqslant \alpha_{i}, \hat{\alpha}_{i} \leqslant C \end{aligned}
\tag{6.51}
```
```math
\left\{\begin{array}{l}{\alpha_{i}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)=0} \\ {\hat{\alpha}_{i}\left(y_{i}-f\left(\boldsymbol{x}_{i}\right)-\epsilon-\hat{\xi}_{i}\right)=0} \\ {\alpha_{i} \hat{\alpha}_{i}=0, \xi_{i} \hat{\xi}_{i}=0} \\ {\left(C-\alpha_{i}\right) \xi_{i}=0,\left(C-\hat{\alpha}_{i}\right) \hat{\xi}_{i}=0}\end{array}\right.
\tag{6.52}
```
同样可以使用核函数的方法:
```math
f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}+b
\tag{6.53}
```
```math
b=y_{i}+\epsilon-\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}
\tag{6.54}
```
```math
\boldsymbol{w}=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \phi\left(\boldsymbol{x}_{i}\right)
\tag{6.55}
```
```math
f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\hat{\alpha}_{i}-\alpha_{i}\right) \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)+b
\tag{6.56}
```
其中`$\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)$`为核函数

## 6.6 核方法
- **定理 6.2 (表示定理)** 令`$\mathbb{H}$`为核函数`$\kappa$`对应的再生核希尔伯特空间, `$\|h\|_{ \mathrm{H}}$`表示`$\mathbb{H}$`空间中关于`$h$`的范数, 对于任意单调递增函数`$\Omega : [0, \infty] \mapsto \mathbb{R}$`和任意非
负损失函数`$\ell : \mathbb{R}^{m} \mapsto[0, \infty]$`, 优化问题:
```math
\min _{h \in \mathbb{H}} F(h)=\Omega\left(\|h\|_{\mathrm{H}}\right)+\ell\left(h\left(\boldsymbol{x}_{1}\right), h\left(\boldsymbol{x}_{2}\right), \ldots, h\left(\boldsymbol{x}_{m}\right)\right)
\tag{6.57}
```
的解可以写为:
```math
h^{*}(\boldsymbol{x})=\sum_{i=1}^{m} \alpha_{i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_{i}\right)
\tag{6.58}
```
表示应理对损失函数没有限制，对正则化项`$\Omega$` 仅要求单调递增，甚至不要求`$\Omega$`是凸函数这 意味着对于般的损失函数和正则化项，优化问题(6.57) 的最优
解`$h^*(x)$` 都可表示为核函数`$\kappa(x, x_i)$`的线性组合, 这显示出核函数的巨大威力.
- LDA 内容参见 [LDA原理](https://blog.csdn.net/ZSZ_shsf/article/details/64123984)